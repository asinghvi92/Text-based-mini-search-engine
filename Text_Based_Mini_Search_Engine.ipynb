{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset provided is southpark_scripts.zip, which includes scripts for episodes of the first twenty seasons of the TV show South Park. You will build an inverted index over these scripts where each episode should be treated as a single document. There are 277 episodes (documents) to index and search on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Parsing\n",
    "\n",
    "Parser provides following two confgiuration options:\n",
    "* Case-folding\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configuration options\n",
    "use_stemming = True # or false\n",
    "use_casefolding = True # or false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For configuration use_stemming = True and use_casefolding = True\n",
      "Unique tokens = 17144\n"
     ]
    }
   ],
   "source": [
    "import glob,os,re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#utility function to speed up stemming \n",
    "stem_cache = {}\n",
    "def cache_based_stemming (token):\n",
    "    if token not in stem_cache:\n",
    "        stem_cache[token] = stemmer.stem(token)        \n",
    "    return stem_cache[token]\n",
    "\n",
    "\n",
    "\n",
    "all_file_content=[]  #will contain the tokens of all documents \n",
    "content = {}\n",
    "\n",
    "# Parses all documents and tokenizes them \n",
    "for file in glob.glob(\"southpark_scripts/*.txt\"):\n",
    "    with open(file) as myfile:\n",
    "        \n",
    "        #'fname' will contain the 4 digit name of the document. Eg, southpark_scripts/0101.txt => fname=0101\n",
    "        file = file.split('\\\\')   \n",
    "        fname = file[-1]\n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()  \n",
    "        \n",
    "        # File reading \n",
    "        content[fname]=  myfile.read()\n",
    "\n",
    "    content[fname] = re.split('\\W+',content[fname])\n",
    "    content[fname] = list(filter(None, content[fname])) \n",
    "    \n",
    "    if use_stemming is True:\n",
    "        for i in range(len(content[fname])):\n",
    "            content[fname][i] = cache_based_stemming(content[fname][i])\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(content[fname])):\n",
    "            content[fname][i] = content[fname][i].lower()\n",
    "            \n",
    "    all_file_content += content[fname]\n",
    "    myfile.close()\n",
    "\n",
    "vocabulary = list(set(all_file_content))\n",
    "\n",
    "print(\"For configuration use_stemming = \"+ str(use_stemming) + \" and use_casefolding = \"+ str(use_casefolding))\n",
    "print(\"Unique tokens = \" + str(len(vocabulary)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Unique tokens with each configurations are - \n",
    "\n",
    "* Stemming + Casefolding       = 17144\n",
    "* Stemming + No Casefolding    = 17368\n",
    "* No Stemming + Casefolding    = 23806\n",
    "* No Stemming + No Casefolding = 29550\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 2: Boolean Retrieval\n",
    "\n",
    "Now we build inverted index to support Boolean retrieval. We only require your index to  support AND queries. e.g., when we query **great again**, we treat it as **great** AND **again**.\n",
    "\n",
    "Example queries:\n",
    "* Rednecks\n",
    "* Troll Trace\n",
    "* Respect my authority\n",
    "* Respect my authoritah\n",
    "* Respected my authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BooleanRetrieval (search_text):\n",
    "    query = search_text.split() \n",
    "\n",
    "    if use_stemming is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = cache_based_stemming(query[i])\n",
    "\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = query[i].lower()\n",
    "\n",
    "    inv_index_dict = {}  #placeholder for inverted index of all documents\n",
    "    query = list(set(query)) #keeping unique items in query \n",
    "    for item in query:\n",
    "        inv_index_dict[item]= []\n",
    "\n",
    "    #query has all the tokens for 'search_text' \n",
    "    #'inv_index_dict' is the inverted index dictionary. 'keys' are the tokens of the 'query', 'values' are list of documnets where those 'item' are present.\n",
    "    #For now, it just has a placeholder for each token of search_text, needs to be populated \n",
    "\n",
    "    ##--------------------------------------------------\n",
    "    for file in glob.glob(\"southpark_scripts/*.txt\"):\n",
    "\n",
    "        file= file.split('\\\\')\n",
    "        fname = file[-1]   \n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()\n",
    "\n",
    "        #intersection of tokens between opened text file ('file') and 'search_text'\n",
    "        common = set(content[fname]) & set(query)\n",
    "        for item in common:\n",
    "            inv_index_dict[item].append(fname)\n",
    "        \n",
    "        myfile.close()\n",
    "\n",
    "   \n",
    "    result = set(inv_index_dict[query[0]] )\n",
    "    for item in query:\n",
    "        result = result & set(inv_index_dict[item])\n",
    "    \n",
    "    result = [ele+'.txt' for ele in result]\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search:hh hh\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search:')\n",
    "# search for the input using your index and print out ids of matching documents\n",
    "print(BooleanRetrieval(search_text))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Observations\n",
    "##TODO\n",
    "When we use stemming and casefolding, is the result different from the result when we do not use them? Do you find cases where you prefer stemming? Or not? Or cases where you prefer casefolding? Or Not? Write down your observations below.\n",
    "\n",
    "Ans) Stemming reduces the word to its root form and Case folding lowers the word's case. The boolean search results vary when they are used against the case when they are not. \n",
    "Typically both will be useful because Boolean search will only either match a document or won't. However, it would hurt the case where we want to search for specific queries and don't want unnecessar extra results. For example, a typical user search for day-to-day  activity would be a good situation for stemming and case folding but an advanced search in, say, some library catalogue shouldn't have stemming and case folding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your observations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PhraseQuerySearch(search_text):\n",
    "    query = search_text.split()\n",
    "\n",
    "    if use_stemming is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = cache_based_stemming(query[i])\n",
    "\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = query[i].lower()\n",
    "\n",
    "    match_doc_result = []\n",
    "    ##--------------------------------------------------\n",
    "    \n",
    "    for file in glob.glob(\"southpark_scripts/*.txt\"):                \n",
    "        file= file.split('\\\\')\n",
    "        fname = file[-1]\n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()\n",
    "\n",
    "        for i in xrange(len(content[fname])):\n",
    "            if content[fname][i] == query[0]: #if there is a match with the first token of query, then proceed to match other tokens consecutively\n",
    "                k = i + 1 \n",
    "                j = 1 \n",
    "                while j<len(query): #matching remaining tokens \n",
    "                    if content[fname][k] == query[j]:\n",
    "                        k = i+1 \n",
    "                        j = j+1 \n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if j==len(query): #there is a match for this document with phrase query \n",
    "                    match_doc_result.append(fname+'.txt')\n",
    "                    break\n",
    "                    \n",
    "        myfile.close()\n",
    "                    \n",
    "    return match_doc_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search (Phrase Query:my authority\n",
      "['0304.txt', '1610.txt']\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search (Phrase Query:')\n",
    "print(PhraseQuerySearch(search_text))\n",
    "# search for the input using your index and print out ids of matching documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Ranked Retrieval\n",
    "\n",
    "In this part, we use the vector space model plus cosine similarity to rank documents.\n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TFIDF scores. That is, use the log-weighted term frequency $1+log(tf)$; and the log-weighted inverse document frequency $log(\\frac{N}{df})$. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For a given query, we rank all the 277 documents and print top 10 scores. For example:\n",
    "\n",
    "* result1 - score1\n",
    "* result2 - score2\n",
    "* result3 - score3\n",
    "* result4 - score4\n",
    "* result5 - score5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def RankRetrieval (search_text):\n",
    "    import glob,os,re,math,operator\n",
    "    import numpy as np\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from scipy import spatial\n",
    "\n",
    "    \n",
    "    ##------------------------------------------------------------------------------------------\n",
    "    #Each element of vocabulary serves as a 'key' in dictionary and the 'value' is the list of count for all documents \n",
    "    #Eg, count_lookup_vocab['The']=[10, 20, 11, ....]   => the token 'The' appears 10 times in document in 0101.txt, 20 times in 0102.txt and so on \n",
    "\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    #Step 1: creating empty dict 'count_lookup_vocab'\n",
    "    count_lookup_vocab = {}\n",
    "    \n",
    "    for item in vocabulary:\n",
    "        count_lookup_vocab[item] = [0 for x in xrange(277)]           #creating placeholder for 277 documents \n",
    "\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    #Step 2: populating 'count_lookup_vocab' dictionary : preprocessing \n",
    "\n",
    "    cosine_score ={}  # will contain cosine scores of all documents\n",
    "\n",
    "    file_index = -1 \n",
    "    file_names=[]  # will contain name of all .txt files\n",
    "    for file in sorted(glob.glob(\"southpark_scripts/*.txt\")):\n",
    "        file_index +=1 \n",
    "\n",
    "        file  = file.split('\\\\')\n",
    "        fname = file[-1]   \n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()\n",
    "\n",
    "        for item in content[fname]:        #every 'item' in 'content' belongs to 'vocabulary'\n",
    "            count_lookup_vocab[item][file_index] +=1  \n",
    "        \n",
    "        file_names.append(fname)\n",
    "        \n",
    "        cosine_score[fname] = []\n",
    "\n",
    "    ##---------------------------------------------------------------------------------------------\n",
    "    #Step 3: 'search_text' tokenization \n",
    "    N = len(file_names)\n",
    "    \n",
    "    temp_doc_content = []\n",
    "    sorted_cosine_score ={}\n",
    "    \n",
    "    query = search_text.split() \n",
    "\n",
    "    if use_stemming is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = cache_based_stemming(query[i])\n",
    "\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = query[i].lower()\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    #Step 4: create TF-IDF score vector (doc_tfidf_vector) for each document using the count_lookup_vocab \n",
    "    #         and computing the cosine scores\n",
    "    \n",
    "    for doc_name in cosine_score:\n",
    "        doc_number = file_names.index(doc_name) \n",
    "        \n",
    "        #reseting\n",
    "        doc_tfidf_vector =[]  \n",
    "        query_tfidf_vector= []  \n",
    "        temp_doc_content =[]\n",
    "        \n",
    "        temp_doc_content = list(set(content[doc_name]))\n",
    "        for token in temp_doc_content:\n",
    "            log_tf= 1+math.log10(count_lookup_vocab[token][doc_number])\n",
    "                \n",
    "            N = len(file_names)\n",
    "            df = sum([1 for ele in count_lookup_vocab[token] if ele!=0])\n",
    "            log_idf = math.log10(float(N)/df)\n",
    "                \n",
    "            doc_tfidf_vector.append(log_tf*log_idf)\n",
    "            \n",
    "            query_tfidf_vector.append(query.count(token))\n",
    "            \n",
    "        if sum(doc_tfidf_vector)==0 or sum(query_tfidf_vector)==0:\n",
    "            sorted_cosine_score[doc_name+'.txt'] = 0 \n",
    "        else:\n",
    "            sorted_cosine_score[doc_name+'.txt'] = (1- spatial.distance.cosine(doc_tfidf_vector, query_tfidf_vector))\n",
    "            \n",
    "    sorted_cosine_score = sorted(sorted_cosine_score.items(), key=operator.itemgetter(1),reverse= True)\n",
    " \n",
    "    return sorted_cosine_score\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_text = raw_input('Ranked Search:')\n",
    "sorted_cosine_score =RankRetrieval(search_text)\n",
    "\n",
    "print(\"Top 10 matches with respective cosine scores are- \")\n",
    "for i in xrange(10):\n",
    "    if sorted_cosine_score[0][1]==0:\n",
    "        print(\"No document match for the query\")\n",
    "        break\n",
    "    elif sorted_cosine_score[i][1]==0:\n",
    "        break \n",
    "    else:\n",
    "        print(sorted_cosine_score[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Done as a part of CS670 course assignment at Texas A&M University "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
