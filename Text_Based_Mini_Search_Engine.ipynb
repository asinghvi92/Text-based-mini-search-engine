{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 1:  Retrieval Models: Boolean + Vector Space\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Thursday, February 2 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as lastname_firstinitial_hw#.ipynb. For example, my homework submission would be: caverlee_j_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Friday, February 5 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We provide the dataset, [southpark_scripts.zip](https://www.dropbox.com/s/6rzfsbn97s8vwof/southpark_scripts.zip), which includes scripts for episodes of the first twenty seasons of the TV show South Park. You will build an inverted index over these scripts where each episode should be treated as a single document. There are 277 episodes (documents) to index and search on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (20 points) Part 1: Parsing\n",
    "\n",
    "First, you should tokenize documents using **whitespaces and punctuations as delimiters** but do not remove stop words. Your parser needs to also provide the following two confgiuration options:\n",
    "* Case-folding\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configuration options\n",
    "use_stemming = True # or false\n",
    "use_casefolding = True # or false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For configuration use_stemming = True and use_casefolding = True\n",
      "Unique tokens = 17144\n"
     ]
    }
   ],
   "source": [
    "import glob,os,re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#utility function to speed up stemming \n",
    "stem_cache = {}\n",
    "def cache_based_stemming (token):\n",
    "    if token not in stem_cache:\n",
    "        stem_cache[token] = stemmer.stem(token)        \n",
    "    return stem_cache[token]\n",
    "\n",
    "\n",
    "\n",
    "all_file_content=[]  #will contain the tokens of all documents \n",
    "content = {}\n",
    "\n",
    "# Parses all documents and tokenizes them \n",
    "for file in glob.glob(\"southpark_scripts/*.txt\"):\n",
    "    with open(file) as myfile:\n",
    "        \n",
    "        #'fname' will contain the 4 digit name of the document. Eg, southpark_scripts/0101.txt => fname=0101\n",
    "        file = file.split('\\\\')   \n",
    "        fname = file[-1]\n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()  \n",
    "        \n",
    "        # File reading \n",
    "        content[fname]=  myfile.read()\n",
    "\n",
    "    content[fname] = re.split('\\W+',content[fname])\n",
    "    content[fname] = list(filter(None, content[fname])) \n",
    "    \n",
    "    if use_stemming is True:\n",
    "        for i in range(len(content[fname])):\n",
    "            content[fname][i] = cache_based_stemming(content[fname][i])\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(content[fname])):\n",
    "            content[fname][i] = content[fname][i].lower()\n",
    "            \n",
    "    all_file_content += content[fname]\n",
    "    myfile.close()\n",
    "\n",
    "vocabulary = list(set(all_file_content))\n",
    "\n",
    "print(\"For configuration use_stemming = \"+ str(use_stemming) + \" and use_casefolding = \"+ str(use_casefolding))\n",
    "print(\"Unique tokens = \" + str(len(vocabulary)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Once you have your parser working, you should report here the size of your dictionary under the four cases. That is, how many unique tokens do you have with stemming on and casefolding on? And so on. You should fill in the following\n",
    "\n",
    "* Stemming + Casefolding       = 17144\n",
    "* Stemming + No Casefolding    = 17368\n",
    "* No Stemming + Casefolding    = 23806\n",
    "* No Stemming + No Casefolding = 29550\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 2: Boolean Retrieval\n",
    "\n",
    "In this part you build an inverted index to support Boolean retrieval. We only require your index to  support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **great again**, your search engine should treat it as **great** AND **again**.\n",
    "\n",
    "Example queries:\n",
    "* Rednecks\n",
    "* Troll Trace\n",
    "* Respect my authority\n",
    "* Respect my authoritah\n",
    "* Respected my authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BooleanRetrieval (search_text):\n",
    "    query = search_text.split() \n",
    "\n",
    "    if use_stemming is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = cache_based_stemming(query[i])\n",
    "\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = query[i].lower()\n",
    "\n",
    "    inv_index_dict = {}  #placeholder for inverted index of all documents\n",
    "    query = list(set(query)) #keeping unique items in query \n",
    "    for item in query:\n",
    "        inv_index_dict[item]= []\n",
    "\n",
    "    #query has all the tokens for 'search_text' \n",
    "    #'inv_index_dict' is the inverted index dictionary. 'keys' are the tokens of the 'query', 'values' are list of documnets where those 'item' are present.\n",
    "    #For now, it just has a placeholder for each token of search_text, needs to be populated \n",
    "\n",
    "    ##--------------------------------------------------\n",
    "    for file in glob.glob(\"southpark_scripts/*.txt\"):\n",
    "\n",
    "        file= file.split('\\\\')\n",
    "        fname = file[-1]   \n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()\n",
    "\n",
    "        #intersection of tokens between opened text file ('file') and 'search_text'\n",
    "        common = set(content[fname]) & set(query)\n",
    "        for item in common:\n",
    "            inv_index_dict[item].append(fname)\n",
    "        \n",
    "        myfile.close()\n",
    "\n",
    "   \n",
    "    result = set(inv_index_dict[query[0]] )\n",
    "    for item in query:\n",
    "        result = result & set(inv_index_dict[item])\n",
    "    \n",
    "    result = [ele+'.txt' for ele in result]\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search:hh hh\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search:')\n",
    "# search for the input using your index and print out ids of matching documents\n",
    "print(BooleanRetrieval(search_text))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Observations\n",
    "##TODO\n",
    "When we use stemming and casefolding, is the result different from the result when we do not use them? Do you find cases where you prefer stemming? Or not? Or cases where you prefer casefolding? Or Not? Write down your observations below.\n",
    "\n",
    "Ans) Stemming reduces the word to its root form and Case folding lowers the word's case. The boolean search results vary when they are used against the case when they are not. \n",
    "Typically both will be useful because Boolean search will only either match a document or won't. However, it would hurt the case where we want to search for specific queries and don't want unnecessar extra results. For example, a typical user search for day-to-day  activity would be a good situation for stemming and case folding but an advanced search in, say, some library catalogue shouldn't have stemming and case folding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your observations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Phrase Queries\n",
    "\n",
    "Your search engine needs to also (optionally) support phrase queries of arbitrary length. Use quotes in a query to tell your search engine this is a phrase query. Again, we don't explicitly type AND in queries and never use OR, NOT, or parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PhraseQuerySearch(search_text):\n",
    "    query = search_text.split()\n",
    "\n",
    "    if use_stemming is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = cache_based_stemming(query[i])\n",
    "\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = query[i].lower()\n",
    "\n",
    "    match_doc_result = []\n",
    "    ##--------------------------------------------------\n",
    "    \n",
    "    for file in glob.glob(\"southpark_scripts/*.txt\"):                \n",
    "        file= file.split('\\\\')\n",
    "        fname = file[-1]\n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()\n",
    "\n",
    "        for i in xrange(len(content[fname])):\n",
    "            if content[fname][i] == query[0]: #if there is a match with the first token of query, then proceed to match other tokens consecutively\n",
    "                k = i + 1 \n",
    "                j = 1 \n",
    "                while j<len(query): #matching remaining tokens \n",
    "                    if content[fname][k] == query[j]:\n",
    "                        k = i+1 \n",
    "                        j = j+1 \n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if j==len(query): #there is a match for this document with phrase query \n",
    "                    match_doc_result.append(fname+'.txt')\n",
    "                    break\n",
    "                    \n",
    "        myfile.close()\n",
    "                    \n",
    "    return match_doc_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search (Phrase Query:my authority\n",
      "['0304.txt', '1610.txt']\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search (Phrase Query:')\n",
    "print(PhraseQuerySearch(search_text))\n",
    "# search for the input using your index and print out ids of matching documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 3: Ranked Retrieval\n",
    "\n",
    "In this part, your job is to support queries over an index that you build. This time you will use the vector space model plus cosine similarity to rank documents.\n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TFIDF scores. That is, use the log-weighted term frequency $1+log(tf)$; and the log-weighted inverse document frequency $log(\\frac{N}{df})$. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For a given query, you should rank all the 277 documents but you only need to output the top-5 documents (i.e. document ids) plus the cosine score of each of these documents. For example:\n",
    "\n",
    "* result1 - score1\n",
    "* result2 - score2\n",
    "* result3 - score3\n",
    "* result4 - score4\n",
    "* result5 - score5\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def RankRetrieval (search_text):\n",
    "    import glob,os,re,math,operator\n",
    "    import numpy as np\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from scipy import spatial\n",
    "\n",
    "    \n",
    "    ##------------------------------------------------------------------------------------------\n",
    "    #Each element of vocabulary serves as a 'key' in dictionary and the 'value' is the list of count for all documents \n",
    "    #Eg, count_lookup_vocab['The']=[10, 20, 11, ....]   => the token 'The' appears 10 times in document in 0101.txt, 20 times in 0102.txt and so on \n",
    "\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    #Step 1: creating empty dict 'count_lookup_vocab'\n",
    "    count_lookup_vocab = {}\n",
    "    \n",
    "    for item in vocabulary:\n",
    "        count_lookup_vocab[item] = [0 for x in xrange(277)]           #creating placeholder for 277 documents \n",
    "\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    #Step 2: populating 'count_lookup_vocab' dictionary : preprocessing \n",
    "\n",
    "    cosine_score ={}  # will contain cosine scores of all documents\n",
    "\n",
    "    file_index = -1 \n",
    "    file_names=[]  # will contain name of all .txt files\n",
    "    for file in sorted(glob.glob(\"southpark_scripts/*.txt\")):\n",
    "        file_index +=1 \n",
    "\n",
    "        file  = file.split('\\\\')\n",
    "        fname = file[-1]   \n",
    "        fname = re.match(r'\\d{4}',fname)\n",
    "        fname = fname.group()\n",
    "\n",
    "        for item in content[fname]:        #every 'item' in 'content' belongs to 'vocabulary'\n",
    "            count_lookup_vocab[item][file_index] +=1  \n",
    "        \n",
    "        file_names.append(fname)\n",
    "        \n",
    "        cosine_score[fname] = []\n",
    "\n",
    "    ##---------------------------------------------------------------------------------------------\n",
    "    #Step 3: 'search_text' tokenization \n",
    "    N = len(file_names)\n",
    "    \n",
    "    temp_doc_content = []\n",
    "    sorted_cosine_score ={}\n",
    "    \n",
    "    query = search_text.split() \n",
    "\n",
    "    if use_stemming is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = cache_based_stemming(query[i])\n",
    "\n",
    "    if use_casefolding is True:\n",
    "        for i in range(len(query)):\n",
    "            query[i] = query[i].lower()\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    #Step 4: create TF-IDF score vector (doc_tfidf_vector) for each document using the count_lookup_vocab \n",
    "    #         and computing the cosine scores\n",
    "    \n",
    "    for doc_name in cosine_score:\n",
    "        doc_number = file_names.index(doc_name) \n",
    "        \n",
    "        #reseting\n",
    "        doc_tfidf_vector =[]  \n",
    "        query_tfidf_vector= []  \n",
    "        temp_doc_content =[]\n",
    "        \n",
    "        temp_doc_content = list(set(content[doc_name]))\n",
    "        for token in temp_doc_content:\n",
    "            log_tf= 1+math.log10(count_lookup_vocab[token][doc_number])\n",
    "                \n",
    "            N = len(file_names)\n",
    "            df = sum([1 for ele in count_lookup_vocab[token] if ele!=0])\n",
    "            log_idf = math.log10(float(N)/df)\n",
    "                \n",
    "            doc_tfidf_vector.append(log_tf*log_idf)\n",
    "            \n",
    "            query_tfidf_vector.append(query.count(token))\n",
    "            \n",
    "        if sum(doc_tfidf_vector)==0 or sum(query_tfidf_vector)==0:\n",
    "            sorted_cosine_score[doc_name+'.txt'] = 0 \n",
    "        else:\n",
    "            sorted_cosine_score[doc_name+'.txt'] = (1- spatial.distance.cosine(doc_tfidf_vector, query_tfidf_vector))\n",
    "            \n",
    "    sorted_cosine_score = sorted(sorted_cosine_score.items(), key=operator.itemgetter(1),reverse= True)\n",
    " \n",
    "    return sorted_cosine_score\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 matches with respective cosine scores are- \n",
      "('1402.txt', 0.04683562634556393)\n",
      "('0904.txt', 0.046774750980709268)\n",
      "('1204.txt', 0.041259329368012909)\n",
      "('0304.txt', 0.039222976908189988)\n",
      "('1005.txt', 0.037307013977343817)\n",
      "('0503.txt', 0.036991702745555743)\n",
      "('1610.txt', 0.035638042526848879)\n",
      "('2004.txt', 0.034594743613401091)\n",
      "('0314.txt', 0.032964087233073203)\n",
      "('1907.txt', 0.032797519955782284)\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Ranked Search:')\n",
    "sorted_cosine_score =RankRetrieval(search_text)\n",
    "\n",
    "print(\"Top 5 matches with respective cosine scores are- \")\n",
    "for i in xrange(10):\n",
    "    if sorted_cosine_score[0][1]==0:\n",
    "        print(\"No document match for the query\")\n",
    "        break\n",
    "    elif sorted_cosine_score[i][1]==0:\n",
    "        break \n",
    "    else:\n",
    "        print(sorted_cosine_score[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grader will randomly pick 5-10 queries to test your program. You are welcome to discuss the results returned by your search engine with others on Piazza."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
